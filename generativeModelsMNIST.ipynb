{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gmYqeSih338l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from the CSV file into a numpy array\n",
        "data = pd.read_csv('MNIST_train.csv')"
      ],
      "metadata": {
        "id": "C0XFPXoI4Sym"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRQtKcTQ5n9e",
        "outputId": "f8fcf4c4-e44d-49a7-b5e8-83658c29b7c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method DataFrame.info of        Unnamed: 0  index  labels  0  1  2  3  4  5  6  ...  774  775  776  \\\n",
            "0               0      0       5  0  0  0  0  0  0  0  ...    0    0    0   \n",
            "1               1      1       0  0  0  0  0  0  0  0  ...    0    0    0   \n",
            "2               2      2       4  0  0  0  0  0  0  0  ...    0    0    0   \n",
            "3               3      3       1  0  0  0  0  0  0  0  ...    0    0    0   \n",
            "4               4      4       9  0  0  0  0  0  0  0  ...    0    0    0   \n",
            "...           ...    ...     ... .. .. .. .. .. .. ..  ...  ...  ...  ...   \n",
            "59995       59995  59995       8  0  0  0  0  0  0  0  ...    0    0    0   \n",
            "59996       59996  59996       3  0  0  0  0  0  0  0  ...    0    0    0   \n",
            "59997       59997  59997       5  0  0  0  0  0  0  0  ...    0    0    0   \n",
            "59998       59998  59998       6  0  0  0  0  0  0  0  ...    0    0    0   \n",
            "59999       59999  59999       8  0  0  0  0  0  0  0  ...    0    0    0   \n",
            "\n",
            "       777  778  779  780  781  782  783  \n",
            "0        0    0    0    0    0    0    0  \n",
            "1        0    0    0    0    0    0    0  \n",
            "2        0    0    0    0    0    0    0  \n",
            "3        0    0    0    0    0    0    0  \n",
            "4        0    0    0    0    0    0    0  \n",
            "...    ...  ...  ...  ...  ...  ...  ...  \n",
            "59995    0    0    0    0    0    0    0  \n",
            "59996    0    0    0    0    0    0    0  \n",
            "59997    0    0    0    0    0    0    0  \n",
            "59998    0    0    0    0    0    0    0  \n",
            "59999    0    0    0    0    0    0    0  \n",
            "\n",
            "[60000 rows x 787 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we first load the MNIST dataset using pandas library and extract the pixel values and labels into separate variables. Then, we normalize the pixel values by dividing them by 255.0, which scales them to a range between 0 and 1. Finally, we verify the normalization results by printing the minimum and maximum pixel values in the normalized dataset."
      ],
      "metadata": {
        "id": "9yNgluJAEXuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract pixel values and labels\n",
        "X_train = data.iloc[:, 1:].values\n",
        "y_train = data.iloc[:, 0].values"
      ],
      "metadata": {
        "id": "mobhadQE8qgn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize pixel values\n",
        "X_train = X_train.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "Zm75kJ_y-T20"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the normalization results\n",
        "print('Min pixel value:', np.min(X_train))\n",
        "print('Max pixel value:', np.max(X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGvwzKF8-8TI",
        "outputId": "1d35abc3-25b3-4c60-e1c0-4d562191448d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min pixel value: 0.0\n",
            "Max pixel value: 235.29019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA stands for Principal Component Analysis.\n",
        "\n",
        "we use the PCA class from the sklearn.decomposition module to perform PCA with the n_components parameter set to 50. This means that the dimensionality of the data will be reduced to 50 features. The fit_transform method is then used to fit the PCA model to the data and transform it into the reduced feature space."
      ],
      "metadata": {
        "id": "cPvnWS_AD1Iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform PCA to reduce dimensionality\n",
        "pca = PCA(n_components=50)  # Set the number of components to keep\n",
        "X_train_pca = pca.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "0uHLgwuxDTkZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the results\n",
        "print('Original shape:', X_train.shape)\n",
        "print('Reduced shape:', X_train_pca.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_rQKddsDnoB",
        "outputId": "5fefeb31-2431-4e73-c91e-f8a0015b7e50"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (60000, 786)\n",
            "Reduced shape: (60000, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate new images using data augmentation, you can use the ImageDataGenerator class from the keras.preprocessing.image module. Here's an example code snippet that applies random rotation, width shift, and height shift to the images:"
      ],
      "metadata": {
        "id": "qdyI2b_REbgI"
      }
    }
  ]
}